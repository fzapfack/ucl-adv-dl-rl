# -*- coding: utf-8 -*-
"""DL_assignment_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18Y-u-jhpIdWXV6un7ptoDWfBbuT1LpF0

# Homework 2

**Start date:** *29th January 2018*

**Due date:** *11th February 2018, 11:55 pm*

## How to Submit

When you have completed the exercises and everything has finsihed running, click on 'File' in the menu-bar and then 'Download .ipynb'. This file must be submitted to Moodle named as **studentnumber_DL_hw2.ipynb** before the deadline above.

Also send a **sharable link** to the notebook at the following email: ucl.coursework.submit@gmail.com. You can also make it sharable via link to everyone, up to you.

### IMPORTANT
Please make sure you submission includes **all results/plots/tables** required for grading. We will not re-run your code.

## The Data

### Handwritten Digit Recognition Dataset (MNIST)

In this assignment we will be using the [MNIST digit dataset](https://yann.lecun.com/exdb/mnist/). 

The dataset contains images of hand-written digits ($0-9$), and the corresponding labels. 

The images have a resolution of $28\times 28$ pixels.

### The MNIST Dataset in TensorFlow

You can use the tensorflow build-in functionality to download and import the dataset into python (see *Setup* section below). But note that this will be the only usage of TensorFlow in this assignment.

## The Assignment

### Objectives

This assignment will be mirroring the first assignment (DL_hw1), but this time you are **not allowed to use any of the Tensorflow functionality for specifing nor optimizing** your neural network models. 
You will now use your **own implementations** of different neural network models (labelled Model 1-4, and described in the corresponding sections of the Colab).

As before, you will train these models to classify hand written digits from the Mnist dataset. 

Keep in mind, the purpose of this exercise is to implement and optimize your own neural networks
architectures without the toolbox/library tailored to do so. This also means, in order to train and evaluate your models, you will need to implement your own optimization procedure. You are to use the same cross-entropy loss as before and your own implementation of SGD.

#### Additional instruction:

**Do not use any other libraries than the ones provided in the imports cell.** You should be able to do everything via *numpy* (especially for the convolutional layer, rely on the in-built matrix/tensor multiplication that numpy offers).  

There are a few questions at the end of the colab. **Before doing any coding, please take look at Question 1** -- this should help you with the implementations, especially the optimization part.


### Variable Initialization

Initialize the variables containing the parameters using [Xavier initialization](http://proceedings.mlr.press/v9/glorot10a.html).

### Hyper-parameters

For each of these models you will be requested to run experiments with different hyper-parameters.

More specifically, you will be requested to try 3 sets of hyper-parameters per model, and report the resulting model accuracy.

Each combination of hyper-parameter will specify how to set each of the following:

- **num_epochs**: Number of iterations through the training section of the dataset [*a positive integer*].

- **learning_rate**: Learning rate used by the gradient descent optimizer [*a scalar between 0 and 1*]

In all experiments use a *batch_size* of 100.

### Loss function
All models, should be trained as to minimize the **cross-entropy loss** function:
$$
\mathrm{loss}
~~=~~
-\sum_{i=1}^N \log p(y_i|x_i, \theta)
~~=~~
-\sum_{i=1}^N \log{ \underbrace{\left(\frac{\exp(z_{i}[y_i])}{\sum_{c=1}^{10} \exp(z_{i}[c])}\right)}_{\text{softmax output}}}
~~=~~
\sum_{i=1}^N \left( -z_{i}[y_i] + \log{\left( \sum_{c=1}^{10} \exp(z_{i}[c]) \right)} \right)$$
where $z \in \mathbb{R}^{10}$ is the input to the softmax layer and $z{[c]}$ denotes the $c$-th entry of vector $z$. And $i$ is a index for the dataset $\{(x_i, y_i)\}_{i=1}^N$.


### Optimization

Use **stochastic gradient descent (SGD)** for optimizing the loss function. Sum over the batch.


### Training and Evaluation

The tensorflow built-in functionality for downloading and importing the dataset into python returns a Datasets object.

This object will have three attributes: 

- train

- validation

- test

Use only the **train** data in order to optimize the model.

Use *datasets.train.next_batch(100)* in order to sample mini-batches of data.

Every 20000 training samples (i.e. every 200 updates to the model), interrupt training and measure the accuracy of the model, 

each time evaluate the accuracy of the model both on 20% of the **train** set and on the entire **test** set.

### Reporting

For each model i, you will collect the learning curves associated to each combination of hyper-parameters.

Use the utility function `plot_learning_curves` to plot these learning curves,

and the and utility function `plot_summary_table` to generate a summary table of results.

For each run collect the train and test curves in a tuple, together with the hyper-parameters.

    experiments_task_i = [

       (num_epochs_1, learning_rate_1), train_accuracy_1, test_accuracy_1),
    
       (num_epochs_2, learning_rate_2), train_accuracy_2, test_accuracy_2),
    
       (num_epochs_3, learning_rate_3), train_accuracy_3, test_accuracy_3)]


### Hint: 

Remind yourselves of the chain rule and read through the lecture notes on back-propagation (computing 
the gradients by recursively applying the chain rule). This is a general procedure that applies to all model
architectures you will have to code in the following steps. Thus, you are strongly encourage to implement an
optimization procedure that generalizes and can be re-used to train all your models. Recall the only things
that you will need for each layer are: 

(i) the gradients of layer with respect to its input

(ii) the gradients with respect to its parameters, if any.

(See Question 1).


Also from the previous assignment, you should have a good idea of what to expect, both terms of behavior and relative performance. (To keep everything comparable, we kept all the hyperparameters and reporting the same).

# Imports and utility functions (do not modify!)
"""

# Import useful libraries.
import matplotlib.pyplot as plt
from tensorflow.examples.tutorials.mnist import input_data
import numpy as np
import tensorflow as tf

# Global variables.
log_period_samples = 20000
batch_size = 100


# Import dataset with one-hot encoding of the class labels.
def get_data():
    return input_data.read_data_sets("MNIST_data/", one_hot=True)


# Placeholders to feed train and test data into the graph.
# Since batch dimension is 'None', we can reuse them both for train and eval.
def get_placeholders():
    x = tf.placeholder(tf.float32, [None, 784])
    y_ = tf.placeholder(tf.float32, [None, 10])
    return x, y_


# Plot learning curves of experiments
def plot_learning_curves(experiment_data):
    # Generate figure.
    fig, axes = plt.subplots(3, 4, figsize=(22, 12))
    st = fig.suptitle(
        "Learning Curves for all Tasks and Hyper-parameter settings",
        fontsize="x-large")
    # Plot all learning curves.
    for i, results in enumerate(experiment_data):
        for j, (setting, train_accuracy, test_accuracy) in enumerate(results):
            # Plot.
            xs = [x * log_period_samples for x in range(1, len(train_accuracy) + 1)]
            axes[j, i].plot(xs, train_accuracy, label='train_accuracy')
            axes[j, i].plot(xs, test_accuracy, label='test_accuracy')
            # Prettify individual plots.
            axes[j, i].ticklabel_format(style='sci', axis='x', scilimits=(0, 0))
            axes[j, i].set_xlabel('Number of samples processed')
            axes[j, i].set_ylabel('Epochs: {}, Learning rate: {}.  Accuracy'.format(*setting))
            axes[j, i].set_title('Task {}'.format(i + 1))
            axes[j, i].legend()
    # Prettify overall figure.
    plt.tight_layout()
    st.set_y(0.95)
    fig.subplots_adjust(top=0.91)
    plt.show()


# Generate summary table of results.
def plot_summary_table(experiment_data):
    # Fill Data.
    cell_text = []
    rows = []
    columns = ['Setting 1', 'Setting 2', 'Setting 3']
    for i, results in enumerate(experiment_data):
        rows.append('Model {}'.format(i + 1))
        cell_text.append([])
        for j, (setting, train_accuracy, test_accuracy) in enumerate(results):
            cell_text[i].append(test_accuracy[-1])
    # Generate Table.
    fig = plt.figure(frameon=False)
    ax = plt.gca()
    the_table = ax.table(
        cellText=cell_text,
        rowLabels=rows,
        colLabels=columns,
        loc='center')
    the_table.scale(1, 4)
    # Prettify.
    ax.patch.set_facecolor('None')
    ax.xaxis.set_visible(False)
    ax.yaxis.set_visible(False)


class Layer(object):
    def __init__(self):
        pass

    def forward(self, x):
        raise NotImplementedError

    def backward(self, dL_dy):
        raise NotImplementedError

    def param_gradients(self, dL_dy):
        raise NotImplementedError


class Linear(Layer):
    def __init__(self,
                 input_units,
                 output_units,
                 bias=True):
        super(Linear, self).__init__()
        self.input_units = input_units
        self.output_units = output_units
        self.weights = np.random.randn(output_units, input_units)
        self.bias = np.zeros(shape=output_units)

    def forward(self, x):
        return np.matmul(self.weights, x) + self.bias

    def backward(self, dL_dy):
        pass

    def param_gradients(self, dL_dy):
        pass


class Add(Layer):
    def __init__(self,
                 input_units,
                 output_units,
                 bias=True):
        super(Add, self).__init__()
        self.input_units = input_units
        self.output_units = output_units
        self.bias = np.zeros(shape=input_units)

    def forward(self, x):
        return x + self.bias

    def backward(self, dL_dy):
        pass

    def param_gradients(self, dL_dy):
        pass


l = Multiply(input_units=100,
             output_units=50)




# """# Model 1 (10 pts)
#
# ### Model
#
# Train a neural network model consisting of 1 linear layer, followed by a softmax:
#
# (input $\rightarrow$ linear layer $\rightarrow$ softmax $\rightarrow$ class probabilities)
#
# ### Hyper-parameters
#
# Train the model with three different hyper-parameter settings:
#
# - *num_epochs*=5, *learning_rate*=0.0001
#
# - *num_epochs*=5, *learning_rate*=0.005
#
# - *num_epochs*=5, *learning_rate*=0.1
# """
#
# # CAREFUL: Running this CL resets the experiments_task1 dictionary where results should be stored.
# # Store results of runs with different configurations in a dictionary.
# # Use a tuple (num_epochs, learning_rate) as keys, and a tuple (training_accuracy, testing_accuracy)
# experiments_task1 = []
# settings = [(5, 0.0001), (5, 0.005), (5, 0.1)]
#
# print('Training Model 1')
#
# # Train Model 1 with the different hyper-parameter settings.
# for (num_epochs, learning_rate) in settings:
#
#   # Reset graph, recreate placeholders and dataset.
#   mnist = get_data()
#   eval_mnist = get_data()
#
#   #####################################################
#   # Define model, loss, update and evaluation metric. #
#   #####################################################
#
#   # Train.
#   i, train_accuracy, test_accuracy = 0, [], []
#   log_period_updates = int(log_period_samples / batch_size)
#   while mnist.train.epochs_completed < num_epochs:
#
#     # Update.
#     i += 1
#     batch_xs, batch_ys = mnist.train.next_batch(batch_size)
#
#     #################
#     # Training step #
#     #################
#
#     # Periodically evaluate.
#     if i % log_period_updates == 0:
#
#       #####################################
#       # Compute and store train accuracy. #
#       #####################################
#
#       #####################################
#       # Compute and store test accuracy.  #
#       #####################################
#
#   experiments_task1.append(
#       ((num_epochs, learning_rate), train_accuracy, test_accuracy))
#
# """# Model 2 (15 pts)
#
# 1 hidden layer (32 units) with a ReLU non-linearity, followed by a softmax.
#
# (input $\rightarrow$ non-linear layer $\rightarrow$ linear layer $\rightarrow$ softmax $\rightarrow$ class probabilities)
#
# ### Hyper-parameters
#
# Train the model with three different hyper-parameter settings:
#
# - *num_epochs*=15, *learning_rate*=0.0001
#
# - *num_epochs*=15, *learning_rate*=0.005
#
# - *num_epochs*=15, *learning_rate*=0.1
# """
#
# # CAREFUL: Running this CL resets the experiments_task1 dictionary where results should be stored.
# # Store results of runs with different configurations in a dictionary.
# # Use a tuple (num_epochs, learning_rate) as keys, and a tuple (training_accuracy, testing_accuracy)
# experiments_task2 = []
# settings = [(15, 0.0001), (15, 0.005), (15, 0.1)]
#
# print('Training Model 2')
#
# # Train Model 1 with the different hyper-parameter settings.
# for (num_epochs, learning_rate) in settings:
#
#   mnist = get_data()  # use for training.
#   eval_mnist = get_data()  # use for evaluation.
#
#   #####################################################
#   # Define model, loss, update and evaluation metric. #
#   #####################################################
#
#   # Train.
#   i, train_accuracy, test_accuracy = 0, [], []
#   log_period_updates = int(log_period_samples / batch_size)
#   while mnist.train.epochs_completed < num_epochs:
#
#     # Update.
#     i += 1
#     batch_xs, batch_ys = mnist.train.next_batch(batch_size)
#
#     #################
#     # Training step #
#     #################
#
#     # Periodically evaluate.
#     if i % log_period_updates == 0:
#
#       #####################################
#       # Compute and store train accuracy. #
#       #####################################
#
#       #####################################
#       # Compute and store test accuracy.  #
#       #####################################
#
#   experiments_task2.append(
#       ((num_epochs, learning_rate), train_accuracy, test_accuracy))
#
# """# Model 3 (15 pts)
#
# 2 hidden layers (32 units) each, with ReLU non-linearity, followed by a softmax.
#
# (input $\rightarrow$ non-linear layer $\rightarrow$ non-linear layer $\rightarrow$ linear layer $\rightarrow$ softmax $\rightarrow$ class probabilities)
#
#
# ### Hyper-parameters
#
# Train the model with three different hyper-parameter settings:
#
# - *num_epochs*=5, *learning_rate*=0.003
#
# - *num_epochs*=40, *learning_rate*=0.003
#
# - *num_epochs*=40, *learning_rate*=0.05
# """
#
# # CAREFUL: Running this CL resets the experiments_task1 dictionary where results should be stored.
# # Store results of runs with different configurations in a dictionary.
# # Use a tuple (num_epochs, learning_rate) as keys, and a tuple (training_accuracy, testing_accuracy)
# experiments_task3 = []
# settings = [(5, 0.003), (40, 0.003), (40, 0.05)]
#
# print('Training Model 3')
#
# # Train Model 1 with the different hyper-parameter settings.
# for (num_epochs, learning_rate) in settings:
#
#   mnist = get_data()  # use for training.
#   eval_mnist = get_data()  # use for evaluation.
#
#   #####################################################
#   # Define model, loss, update and evaluation metric. #
#   #####################################################
#
#   # Train.
#   i, train_accuracy, test_accuracy = 0, [], []
#   log_period_updates = int(log_period_samples / batch_size)
#   while mnist.train.epochs_completed < num_epochs:
#
#     # Update.
#     i += 1
#     batch_xs, batch_ys = mnist.train.next_batch(batch_size)
#
#     #################
#     # Training step #
#     #################
#
#     # Periodically evaluate.
#     if i % log_period_updates == 0:
#
#       #####################################
#       # Compute and store train accuracy. #
#       #####################################
#
#       #####################################
#       # Compute and store test accuracy.  #
#       #####################################
#
#   experiments_task3.append(
#       ((num_epochs, learning_rate), train_accuracy, test_accuracy))
#
# """# Model 4 (20 pts)
#
# ### Model
# 3 layer convolutional model (2 convolutional layers followed by max pooling) + 1 non-linear layer (32 units), followed by softmax.
#
# (input(28x28) $\rightarrow$ conv(3x3x8) + maxpool(2x2) $\rightarrow$ conv(3x3x8) + maxpool(2x2) $\rightarrow$ flatten $\rightarrow$ non-linear $\rightarrow$ linear layer $\rightarrow$ softmax $\rightarrow$ class probabilities)
#
#
# - Use *padding = 'SAME'* for both the convolution and the max pooling layers.
#
# - Employ plain convolution (no stride) and for max pooling operations use 2x2 sliding windows, with no overlapping pixels (note: this operation will down-sample the input image by 2x2).
#
# ### Hyper-parameters
#
# Train the model with three different hyper-parameter settings:
#
# - num_epochs=5, learning_rate=0.01
#
# - num_epochs=10, learning_rate=0.001
#
# - num_epochs=20, learning_rate=0.001
# """
#
# # CAREFUL: Running this CL resets the experiments_task1 dictionary where results should be stored.
# # Store results of runs with different configurations in a dictionary.
# # Use a tuple (num_epochs, learning_rate) as keys, and a tuple (training_accuracy, testing_accuracy)
# experiments_task4 = []
# settings = [(5, 0.01), (10, 0.001), (20, 0.001)]
#
# print('Training Model 4')
#
# # Train Model 1 with the different hyper-parameter settings.
# for (num_epochs, learning_rate) in settings:
#
#   mnist = get_data()  # use for training.
#   eval_mnist = get_data()  # use for evaluation.
#
#   #####################################################
#   # Define model, loss, update and evaluation metric. #
#   #####################################################
#
#   # Train.
#   i, train_accuracy, test_accuracy = 0, [], []
#   log_period_updates = int(log_period_samples / batch_size)
#   while mnist.train.epochs_completed < num_epochs:
#
#     # Update.
#     i += 1
#     batch_xs, batch_ys = mnist.train.next_batch(batch_size)
#
#     #################
#     # Training step #
#     #################
#
#     # Periodically evaluate.
#     if i % log_period_updates == 0:
#
#       #####################################
#       # Compute and store train accuracy. #
#       #####################################
#
#       #####################################
#       # Compute and store test accuracy.  #
#       #####################################
#
#   experiments_task4.append(
#       ((num_epochs, learning_rate), train_accuracy, test_accuracy))
#
# """# Results"""
#
# plot_learning_curves([experiments_task1, experiments_task2, experiments_task3, experiments_task4])
#
# plot_summary_table([experiments_task1, experiments_task2, experiments_task3, experiments_task4])
#
# """# Questions
#
# ###Q1 (32 pts): Compute the following derivatives
# Show all intermediate steps in the derivation (in markdown below). Provide the final results in vector/matrix/tensor form whenever appropiate.
#
# 1. [5 pts] Give the cross-entropy loss above, compute the derivative of the loss function with respect to the scores $z$ (the input to the softmax layer).
# $$\frac{\partial loss}{\partial z} = ?$$
#
# 2. [12 pts] Consider the first model (M1: linear + softmax). Compute the derivative of the loss with respect to
#   * the input $x$
#   $$\frac{\partial loss}{\partial x} = ?$$
#   * the parameters of the linear layer: weights $W$ and bias $b$
#   $$\frac{\partial loss}{\partial W} = ?$$
#   $$\frac{\partial loss}{\partial b} = ?$$
#
# 3. [15 pts] Compute the derivative of a convolution layer wrt. to its parameters W and wrt. to its input (4-dim tensor). Assume a filter of size H x W x D, and stride 1.
#   $$\frac{\partial loss}{\partial W} = ?$$
#
# ### A1: (Your answer here)
#
# ### Q2 (8 pts): How do the results compare to the ones you got when implementing these models in TensorFlow?
# 1. [4 pts] For each of the models, please comment on any differences or discrepancies in results -- runtime, performance and stability in training and final performance. (This would be the place to justify design decisions in the implementation and their effects).
# 2. [2 pts] Which of the models show under-fitting?
# 3. [2 pts] Which of the models show over-fitting?
#
# ### A2: (Your answer here)
# """
#
