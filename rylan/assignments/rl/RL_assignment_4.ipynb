{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYs6LMEbNqoQ",
    "colab_type": "text"
   },
   "source": [
    "# RL homework 4\n",
    "\n",
    "-------------------------------\n",
    "\n",
    "\n",
    "**Name:** Your Name\n",
    "\n",
    "**SN:** Your Student Number\n",
    "\n",
    "-----------------------------------\n",
    "\n",
    "\n",
    "**Start date:** *29th March 2018*\n",
    "\n",
    "**Due date:** *19th April 2018, 4:55 pm*\n",
    "\n",
    "------------------------------------\n",
    "\n",
    "## How to Submit\n",
    "\n",
    "When you have completed the exercises and everything has finsihed running, click on 'File' in the menu-bar and then 'Download .ipynb'. This file must be submitted to Moodle named as **studentnumber_RL_hw4.ipynb** before the deadline above.\n",
    "\n",
    "Also send a **sharable link** to the notebook at the following email: ucl.coursework.submit@gmail.com. You can also make it sharable via link to everyone, up to you.\n",
    "\n",
    "Please compile all results and all answers to the understanding questions into a PDF. Name convention: **studentnumber_RL_hw4.pdf**. Do not include any of the code (we will use the notebook for that). \n",
    "\n",
    "**Page limit: 10 pg **\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9v_SYckYfv5G",
    "colab_type": "text"
   },
   "source": [
    "## Context\n",
    "\n",
    "In this assignment, we will take a first look at learning decisions from data.  For this, we will use the multi-armed bandit framework.\n",
    "\n",
    "## Background reading\n",
    "\n",
    "* Sutton and Barto (2018), Chapters 6-7 + 9-11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztQEQvnKh2t6",
    "colab_type": "text"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qB0tQ4aiAaIu",
    "colab_type": "text"
   },
   "source": [
    "### Import Useful Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YzYtxi8Wh5SJ",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NDhSYfSDcCC",
    "colab_type": "text"
   },
   "source": [
    "### Set options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Ps5OnkPmDbMX",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3, suppress=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOu9RZY3AkF1",
    "colab_type": "text"
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "6EttQGJ1n5Zn",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "def run_experiment(env, agent, number_of_steps):\n",
    "    mean_reward = 0.\n",
    "    try:\n",
    "      action = agent.initial_action()\n",
    "    except AttributeError:\n",
    "      action = 0\n",
    "    for i in range(number_of_steps):\n",
    "      reward, discount, next_state = grid.step(action)\n",
    "      action = agent.step(reward, discount, next_state)\n",
    "      mean_reward += (reward - mean_reward)/(i + 1.)\n",
    "\n",
    "    return mean_reward\n",
    "\n",
    "map_from_action_to_subplot = lambda a: (2, 6, 8, 4)[a]\n",
    "map_from_action_to_name = lambda a: (\"up\", \"right\", \"down\", \"left\")[a]\n",
    "\n",
    "def plot_values(values, colormap='pink', vmin=None, vmax=None):\n",
    "  plt.imshow(values, interpolation=\"nearest\", cmap=colormap, vmin=vmin, vmax=vmax)\n",
    "  plt.yticks([])\n",
    "  plt.xticks([])\n",
    "  plt.colorbar(ticks=[vmin, vmax])\n",
    "\n",
    "def plot_action_values(action_values, vmin=None, vmax=None):\n",
    "  q = action_values\n",
    "  fig = plt.figure(figsize=(8, 8))\n",
    "  fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "  for a in [0, 1, 2, 3]:\n",
    "    plt.subplot(3, 3, map_from_action_to_subplot(a))\n",
    "    plot_values(q[..., a], vmin=vmin, vmax=vmax)\n",
    "    action_name = map_from_action_to_name(a)\n",
    "    plt.title(r\"$q(s, \\mathrm{\" + action_name + r\"})$\")\n",
    "    \n",
    "  plt.subplot(3, 3, 5)\n",
    "  v = 0.9 * np.max(q, axis=-1) + 0.1 * np.mean(q, axis=-1)\n",
    "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\n",
    "  plt.title(\"$v(s)$\")\n",
    "\n",
    "\n",
    "def plot_rewards(xs, rewards, color):\n",
    "  mean = np.mean(rewards, axis=0)\n",
    "  p90 = np.percentile(rewards, 90, axis=0)\n",
    "  p10 = np.percentile(rewards, 10, axis=0)\n",
    "  plt.plot(xs, mean, color=color, alpha=0.6)\n",
    "  plt.fill_between(xs, p90, p10, color=color, alpha=0.3)\n",
    "  \n",
    "\n",
    "def parameter_study(parameter_values, parameter_name,\n",
    "  agent_constructor, env_constructor, color, repetitions=10, number_of_steps=int(1e4)):\n",
    "  mean_rewards = np.zeros((repetitions, len(parameter_values)))\n",
    "  greedy_rewards = np.zeros((repetitions, len(parameter_values)))\n",
    "  for rep in range(repetitions):\n",
    "    for i, p in enumerate(parameter_values):\n",
    "      env = env_constructor()\n",
    "      agent = agent_constructor()\n",
    "      if 'eps' in parameter_name:\n",
    "        agent.set_epsilon(p)\n",
    "      elif 'alpha' in parameter_name:\n",
    "        agent._step_size = p\n",
    "      else:\n",
    "        raise NameError(\"Unknown parameter_name: {}\".format(parameter_name))\n",
    "      mean_rewards[rep, i] = run_experiment(grid, agent, number_of_steps)\n",
    "      agent.set_epsilon(0.)\n",
    "      agent._step_size = 0.\n",
    "      greedy_rewards[rep, i] = run_experiment(grid, agent, number_of_steps//10)\n",
    "      del env\n",
    "      del agent\n",
    "\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plot_rewards(parameter_values, mean_rewards, color)\n",
    "  plt.yticks=([0, 1], [0, 1])\n",
    "  # plt.ylim((0, 1.5))\n",
    "  plt.ylabel(\"Average reward over first {} steps\".format(number_of_steps), size=12)\n",
    "  plt.xlabel(parameter_name, size=12)\n",
    "\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plot_rewards(parameter_values, greedy_rewards, color)\n",
    "  plt.yticks=([0, 1], [0, 1])\n",
    "  # plt.ylim((0, 1.5))\n",
    "  plt.ylabel(\"Final rewards, with greedy policy\".format(number_of_steps), size=12)\n",
    "  plt.xlabel(parameter_name, size=12)\n",
    "  \n",
    "def epsilon_greedy(q_values, epsilon):\n",
    "  if epsilon < np.random.random():\n",
    "    return np.argmax(q_values)\n",
    "  else:\n",
    "    return np.random.randint(np.array(q_values).shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTJ3WYL8Y0GQ",
    "colab_type": "text"
   },
   "source": [
    "### A small MRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "iPnQLBHsYzdq",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "class SmallMRP(object):\n",
    "\n",
    "  def __init__(self, p=0.1):\n",
    "    self._state = 0\n",
    "    self._p = p\n",
    "\n",
    "  def get_state(self):\n",
    "    return self._state\n",
    "\n",
    "  def step(self):\n",
    "    reward = 0\n",
    "    discount = 1\n",
    "    if self._state == 0:\n",
    "      self._state = 1\n",
    "    else:\n",
    "      if np.random.random() < self._p:\n",
    "        self._state = 0\n",
    "        discount = 0\n",
    "      else:\n",
    "        self._state = 1\n",
    "\n",
    "    return reward, discount, self.get_obs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdojBKQ2CK9e",
    "colab_type": "text"
   },
   "source": [
    "## Assignment 1 [50pts in total]\n",
    "\n",
    "We are going to analyze the simple Markov reward process (MRP - a MRP is an MDP without actions or, equivalently, with a single action in each state) defined in the code cell above.  It consists of two states.  The reward is zero everywhere.  When we are in state $s_0$, we always transition to $s_1$.  If we are in state $s_1$, there is a probability $p$ (which is set to 0.1 by default in the code above) of terminating, after which the next episode starts in $s_0$ again.  With a probability of $1 - p$, we transition from $s_1$ back to itself again.  The discount is $\\gamma = 1$ on non-terminal steps.\n",
    "\n",
    "#### [1pt] Question 1.1\n",
    "What is the optimal value in each state?\n",
    "\n",
    "_answer here_\n",
    "\n",
    "#### [1pt] Question 1.2\n",
    "Instead of a tabular representation, consider a single feature $\\phi$, which takes the values $\\phi(s_0) = 1$ and $\\phi(s_1) = 4$.  Now consider using linear function approximation, where we learn a value $\\theta$ such that $v_{\\theta}(s) = \\theta \\times \\phi(s) \\approx v(s)$, where $v(s)$ is the true value of state $s$.  What is the optimal value of $\\theta$?\n",
    "\n",
    "_answer here_\n",
    "\n",
    "#### [8pts] Question 1.3\n",
    "Suppose $\\theta_0 = 1$, and suppose we update this parameter with TD(0) with a step size of $\\alpha = 0.1$.  What is the expected value of $\\mathbb{E}[ \\theta_T ]$ if we step through the MDP until it terminates after the first episode, as a function of $p$?  (Note that $T$ is random.)\n",
    "\n",
    "_answer here_\n",
    "\n",
    "#### [5pts] Question 1.3\n",
    "If $p=0.1$, how many episodes does it take, starting from $\\theta_0 = 1$, until $| v(s) - \\mathbb{E}[v_{\\theta}(s)] | < 0.5$ for all $s$, where the expectation is over the expected updates to $\\theta$?\n",
    "\n",
    "_answer here_\n",
    "\n",
    "#### Synchronous updates\n",
    "Consider the following algorithm: we use TD to update the parameters, but instead of using the online data, we assume we can actively sample a transition from both states.  We then update $\\theta$ using both samples:\n",
    "$$\n",
    "\\theta_{n+1} = \\theta_n + \\alpha \\delta_0 \\phi(s_0) + \\alpha \\delta_1 \\phi(s_1) \\,,\n",
    "$$\n",
    "where $\\delta_i$ is a sampled one-step TD error when transitioning from state $s_i$.\n",
    "\n",
    "#### [10pts] Question 1.4\n",
    "\n",
    "What is the value of $\\mathbb{E}[\\theta_n]$, as a function of $n$ and $p$?\n",
    "\n",
    "_answer here_\n",
    "\n",
    "#### [5pts] Question 1.5\n",
    "\n",
    "For which values of $p$ does not $\\theta$ converge to the optimal solution?\n",
    "\n",
    "_answer here_\n",
    "\n",
    "#### [10pts] Question 1.5\n",
    "Why doesn't it?  TD is known to converge, with linear function approximation, under certain assumptions.  Explain for this concrete case why the algorithm does not converge, and explain which general assumption is violated that would otherwise ensure convergence of linear TD (in at most 200 words).\n",
    "\n",
    "_answer here_\n",
    "\n",
    "#### [10pts] Question 1.5\n",
    "Describe a way to change the algorithm to obtain convergence of $\\theta$, for any $p$, without changing the sampling or the value function (which should remain as $v_{\\theta}(s) = \\theta \\times \\phi(s)$).  Note that the sampling is not sequential, so for instance you cannot add memory of previous states.  (At most 200 words.)\n",
    "\n",
    "_answer here_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4v8_c7XqsEo",
    "colab_type": "text"
   },
   "source": [
    "## Assignment 2 [50pts in total + 10 BONUS pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALrRR76eAd6u",
    "colab_type": "text"
   },
   "source": [
    "### A grid world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "YP97bVN3NuG8",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "class Grid(object):\n",
    "\n",
    "  def __init__(self, tabular=True, vision_size=1, noisy=False):\n",
    "    # -1: wall\n",
    "    # 0: empty, episode continues\n",
    "    # other: number indicates reward, episode will terminate\n",
    "    self._layout = np.array([\n",
    "      [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "      [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "      [-1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1],\n",
    "      [-1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1],\n",
    "      [-1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1],\n",
    "      [-1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1],\n",
    "      [-1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1],\n",
    "      [-1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1],\n",
    "      [-1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1],\n",
    "      [-1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1],\n",
    "      [-1, -1, -1, -5, -1, -1, -1, -1, -6, -1, -1, -1, -1, 10, -1, -1, -1],\n",
    "      [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "      [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "    ])\n",
    "    self._start_state = (3, 2)\n",
    "    self._state = self._start_state\n",
    "    self._number_of_states = np.prod(np.shape(self._layout))\n",
    "    self._noisy = noisy\n",
    "    self._tabular = tabular\n",
    "    self._vision_size = vision_size\n",
    "\n",
    "  @property\n",
    "  def number_of_states(self):\n",
    "      return self._number_of_states\n",
    "    \n",
    "  def plot_grid(self):\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(self._layout != -1, interpolation=\"nearest\")\n",
    "    ax = plt.gca()\n",
    "    ax.grid(0)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"The grid\")\n",
    "    plt.text(2, 2, r\"$\\mathbf{S}$\", ha='center', va='center')\n",
    "    goal_y, goal_x = np.where(self._layout==10)\n",
    "    plt.text(goal_x, goal_y, r\"$\\mathbf{G}$\", ha='center', va='center')\n",
    "    goal_y, goal_x = np.where(self._layout==-5)\n",
    "    plt.text(goal_x, goal_y, r\"$\\mathbf{D}$\", ha='center', va='center')\n",
    "    goal_y, goal_x = np.where(self._layout==-6)\n",
    "    plt.text(goal_x, goal_y, r\"$\\mathbf{D}$\", ha='center', va='center')\n",
    "    h, w = self._layout.shape\n",
    "    for y in range(h-1):\n",
    "      plt.plot([-0.5, w-0.5], [y+0.5, y+0.5], '-k', lw=2)\n",
    "    for x in range(w-1):\n",
    "      plt.plot([x+0.5, x+0.5], [-0.5, h-0.5], '-k', lw=2)\n",
    "\n",
    "  def get_obs(self):\n",
    "    y, x = self._state\n",
    "    return self.get_obs_at(x, y)\n",
    "\n",
    "  def get_obs_at(self, x, y):\n",
    "    if self._tabular:\n",
    "      return y*self._layout.shape[1] + x\n",
    "    else:\n",
    "      v = self._vision_size\n",
    "      location = np.clip(-self._layout[y-v:y+v+1,x-v:x+v+1], 0, 1)\n",
    "      return location\n",
    "\n",
    "  def step(self, action):\n",
    "    y, x = self._state\n",
    "    \n",
    "    if action == 0:  # up\n",
    "      new_state = (y - 1, x)\n",
    "    elif action == 1:  # right\n",
    "      new_state = (y, x + 1)\n",
    "    elif action == 2:  # down\n",
    "      new_state = (y + 1, x)\n",
    "    elif action == 3:  # left\n",
    "      new_state = (y, x - 1)\n",
    "    else:\n",
    "      raise ValueError(\"Invalid action: {} is not 0, 1, 2, or 3.\".format(action))\n",
    "\n",
    "    new_y, new_x = new_state\n",
    "    discount = 0.98\n",
    "    if self._layout[new_y, new_x] == -1:  # wall\n",
    "      reward = -1\n",
    "      new_state = (y, x)\n",
    "    elif self._layout[new_y, new_x] != 0: # a goal\n",
    "      reward = self._layout[new_y, new_x]\n",
    "      discount = 0.\n",
    "      new_state = self._start_state\n",
    "    else:\n",
    "      reward = (new_y + new_x) / np.sum(self._layout.shape)\n",
    "    if self._noisy:\n",
    "      width = self._layout.shape[1]\n",
    "      reward += 2*np.random.normal(0, width - new_x + new_y)\n",
    "    \n",
    "    self._state = new_state\n",
    "\n",
    "    return reward, discount, self.get_obs()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaGeLcsvixmt",
    "colab_type": "text"
   },
   "source": [
    "### The grid\n",
    "\n",
    "The cell below shows the `Grid` environment that we will use. Here `S` indicates the start state and `G` indicates the goal.  The agent has four possible actions: up, right, down, and left.  Rewards are: `-1` for bumping into a wall, `+10` for reaching the goal, and `(x + y)/(height + width)` otherwise, which encourages the agent to go right and down.  The episode ends when the agent reaches the goal.  At the end of the left-most two corridors, there are distractor 'goals' (marked `D`) that give a reward of $-5$ and $-6$, and then also terminate the episode.  The discount, on continuing steps, is $\\gamma = 0.98$.  Feel free to reference the implemetation of the `Grid` above, under the header \"a grid world\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "SlFuWFzIi5uB",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "grid = Grid()\n",
    "grid.plot_grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKfA7ifHvO-M",
    "colab_type": "text"
   },
   "source": [
    "\n",
    "## Implement agents\n",
    "**[10 pts]** In the next code cell, implement an agent that uses **tabular Sarsa** to learn action values.  The agent should act according to an $\\epsilon$-greedy policy with respect to its action values.\n",
    "\n",
    "The agent will be initialized with:\n",
    "```\n",
    "agent = Sarsa(number_of_states=grid._layout.size,\n",
    "              number_of_actions=4,\n",
    "              grid.get_obs())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "u_hLSL8anhsv",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "class Sarsa(object):\n",
    "\n",
    "  def __init__(self, number_of_states, number_of_actions, initial_state, step_size=0.1):\n",
    "    pass\n",
    "    \n",
    "  @property\n",
    "  def q_values(self):\n",
    "    # This function should return the action values for all states and actions\n",
    "    # in a numpy arrar of shape (number_of_states, number_of_actions)\n",
    "    pass\n",
    "\n",
    "  def step(self, r, g, s):\n",
    "    # This function should return an action\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMr_z0RZsHNj",
    "colab_type": "text"
   },
   "source": [
    "**[20 pts]** Implement an agent that uses **neural Sarsa** to learn action values.  The agent should expect a nxn input which it should flatten into a vector, and then pass through a multi-layer perceptron with a single hidden layer with 100 hidden nodes and ReLU activations.  Each weight layer should also have a bias.  Initialize all weights uniformly randomly in $[-0.05, 0.05]$.\n",
    "\n",
    "```\n",
    "NeuralSarsa(number_of_features=(2*vision_size + 1)**2,\n",
    "            number_of_hidden=100,\n",
    "            number_of_actions=4,\n",
    "            initial_state=grid.get_obs(),\n",
    "            step_size=0.01)\n",
    "```\n",
    "\n",
    "The number `vision_size` will be either 1 or 2 below.  The input vector will be of size $(2v + 1)^2$, which will correspond to a square local view of the grid, centered on the agent, and of size $(2v + 1) \\times (2v + 1)$ (so either 3x3 or 5x5).\n",
    "\n",
    "You are allowed, but not mandated, to use TensorFlow to implement this agent.  (The network is small enough that you can also use numpy, but then you have to implement your own backprop.)  Please document the code clearly, especially on non-trivial operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "fppAqbUOn8cO",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "class NeuralSarsa(object):\n",
    "\n",
    "  def __init__(self, number_of_features, number_of_hidden, number_of_actions, initial_state, step_size=0.01):\n",
    "    pass\n",
    "\n",
    "  def q(self, obs):\n",
    "    # This function should give the vector of action values for observation obs\n",
    "  \n",
    "  def step(self, r, g, s):\n",
    "    # This function should return an action\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jZsPzCmDxAh",
    "colab_type": "text"
   },
   "source": [
    "# Analyse Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQkk8sMxE0N4",
    "colab_type": "text"
   },
   "source": [
    "### Run the cells below to train the tabular and neural SARSA agents and to generate plots.\n",
    "\n",
    "This trains the agents the Grid problem with an epsilon of 0.1.\n",
    "\n",
    "The plots below will show action values for each of the actions, as well as a state value defined by $v(s) = \\sum_a \\pi(a|s) q(s, a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "GsNBHNZtHCPe",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    },
    "cellView": "both"
   },
   "outputs": [],
   "source": [
    "grid = Grid()\n",
    "agent = Sarsa(number_of_states=grid._layout.size,\n",
    "              number_of_actions=4,\n",
    "              grid.get_obs())\n",
    "run_experiment(grid, agent, int(1e5))\n",
    "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
    "plot_action_values(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "K-JnvcP0mrxF",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "vision_size = 1\n",
    "grid = Grid(tabular=False, vision_size=vision_size)\n",
    "agent = NeuralSarsa(number_of_features=(2*vision_size + 1)**2,\n",
    "                    number_of_hidden=100,\n",
    "                    number_of_actions=4,\n",
    "                    initial_state=grid.get_obs(),\n",
    "                    step_size=0.01)\n",
    "run_experiment(grid, agent, int(1e5))\n",
    "h, w = grid._layout.shape\n",
    "obs = np.array([[grid.get_obs_at(x, y) for x in range(2, w-2)] for y in range(2, h-2)])\n",
    "qs = np.array([[[agent.q(o)[a] for a in range(4)] if o[vision_size,vision_size] == 0 else np.zeros((4,)) for o in ob] for ob in obs])\n",
    "plot_action_values(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "tmGFDriIPsGz",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "vision_size = 2\n",
    "grid = Grid(tabular=False, vision_size=vision_size)\n",
    "agent = NeuralSarsa(number_of_features=(2*vision_size + 1)**2,\n",
    "                    number_of_hidden=100,\n",
    "                    number_of_actions=4,\n",
    "                    initial_state=grid.get_obs(),\n",
    "                    step_size=0.01)\n",
    "run_experiment(grid, agent, int(1e5))\n",
    "h, w = grid._layout.shape\n",
    "obs = np.array([[grid.get_obs_at(x, y) for x in range(2, w-2)] for y in range(2, h-2)])\n",
    "qs = np.array([[[agent.q(o)[a] for a in range(4)] if o[vision_size,vision_size] == 0 else np.zeros((4,)) for o in ob] for ob in obs])\n",
    "plot_action_values(qs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGptHwE23lmP",
    "colab_type": "text"
   },
   "source": [
    "## Questions\n",
    "\n",
    "Consider the greedy policy with respect to the estimated values\n",
    "\n",
    "**[5 pts]** Which algorithm performed best?  Why?\n",
    "\n",
    "**[5 pts]** Is there a difference in the solution found by Neural Sarsa with a vision size of 1 (so 3x3 local observations), and a vision size of 2 (so 5x5 local observations)?  Why?\n",
    "\n",
    "**[10 pts]** How could we improve the performance of the Neural Sarsa agent on this domain (for both vision sizes)?  Identify the main issue, and propose a concrete solution (in max 200 words).\n",
    "\n",
    "**[10 BONUS pts]** Implement your proposed improvement and show that it actually helps performance."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "RL_assignment_4.ipynb",
   "version": "0.3.2",
   "views": {},
   "default_view": {},
   "provenance": [
    {
     "file_id": "1a1ONHRz5bcd2rJyLUD53OUMR8npSp0QZ",
     "timestamp": 1.522325021849E12
    },
    {
     "file_id": "1Ldj742iIDtvjYKKwENvrpTQ3Hm2wrqIg",
     "timestamp": 1.521476023411E12
    },
    {
     "file_id": "1FwMxkDPkt68fxovrMmmWwm6ohYvX2wt1",
     "timestamp": 1.517660129183E12
    },
    {
     "file_id": "1wwTq5nociiMHUb26jxrvZvGN6l11xV5o",
     "timestamp": 1.517174839485E12
    },
    {
     "file_id": "1_gJNoj9wG4mnigscGRAcZx7RHix3HCjG",
     "timestamp": 1.515086437469E12
    },
    {
     "file_id": "1hcBeMVfaSh8g1R2ujtmxOSHoxJ8xYkaW",
     "timestamp": 1.511098107887E12
    }
   ],
   "collapsed_sections": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
